{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb59438",
   "metadata": {
    "cellId": "jsttyw9fk8tiyeraemiir",
    "collapsed": true,
    "execution_id": "5e968993-ee5e-4a49-b4f4-19ccd0841ede",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c72fd6",
   "metadata": {
    "cellId": "uw03er2f9fht3cy1euvehh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/jupyter/.local/lib/python3.8/site-packages (21.3.1)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2127e24",
   "metadata": {
    "cellId": "0szq8hysb2qhp3qb6kbwfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext in /home/jupyter/.local/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from torchtext) (2.25.1)\n",
      "Requirement already satisfied: numpy in /kernel/lib/python3.8/site-packages (from torchtext) (1.19.4)\n",
      "Collecting torch==1.10.1\n",
      "  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
      "     |████████████████████████████████| 881.9 MB 2.0 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/jupyter/.local/lib/python3.8/site-packages (from torchtext) (4.49.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1->torchtext) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->torchtext) (1.26.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.8/site-packages (from requests->torchtext) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->torchtext) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->torchtext) (2.10)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.1\n",
      "    Uninstalling torch-1.9.1:\n",
      "      Successfully uninstalled torch-1.9.1\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.1+cu111 requires torch==1.9.1, but you have torch 1.10.1 which is incompatible.\n",
      "torchaudio 0.9.1 requires torch==1.9.1, but you have torch 1.10.1 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.10.1\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1268d6f5",
   "metadata": {
    "cellId": "kf0mbq5rlivrpohx82w1i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: razdel in /home/jupyter/.local/lib/python3.8/site-packages (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install razdel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd033eeb",
   "metadata": {
    "cellId": "yfgl042yrrdtr6l3cjypmj"
   },
   "source": [
    "## Цель тетрадки обучить модель и сохранить ее для бота\n",
    "\n",
    "* сначала надо обернуть данные мейл-ру в datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e931b31",
   "metadata": {
    "cellId": "8nmern6jinsxfflac4jxhp"
   },
   "source": [
    "Сначала проредим данные, чтобы не тратить кучу времени на обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ace5a914",
   "metadata": {
    "cellId": "2g91l9unl9s9m53jejkpt6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  train.txt.zip\n",
      "  inflating: train.txt               \n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "!unzip train.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fab95f6",
   "metadata": {
    "cellId": "qsj1e54jdw40o956ypve5"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "!cat train.txt | awk 'BEGIN{i=0} {i+=1; if (i % 50 == 0) {print($0)} ;  }' >> rared_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8a94e3",
   "metadata": {
    "cellId": "fdjpanp5qisszvfbcqbbp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8a992ae",
   "metadata": {
    "cellId": "si18uarcfvk02cmgk93knhy"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class PairsDataset(Dataset):\n",
    "    def __init__(self, file_path, sep='\\t'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        self.sep = sep\n",
    "        self.data = self.load_data()\n",
    "        \n",
    "    def load_data(self): \n",
    "        data = list()\n",
    "        with open(self.file_path) as file_object:\n",
    "            for line in file_object:\n",
    "                data.append(line.strip().split(self.sep))\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4a31a598",
   "metadata": {
    "cellId": "1cfgv2eg3dv2689fq2gytx"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "data = PairsDataset('rared_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f93f8e",
   "metadata": {
    "cellId": "zed7o4vn6agyxqtnf42lm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23522"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "round(0.2 * data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8586e0fd",
   "metadata": {
    "cellId": "5pmv5damq68m5d1ax9zzgb"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "split = 0.2\n",
    "test_data = data[:round(split * data.__len__())]\n",
    "train_data = data[round(split * data.__len__()):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "501372be",
   "metadata": {
    "cellId": "8wc5w2ax6xnjhiykp3o5hm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94087"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_data.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f23f6",
   "metadata": {
    "cellId": "vqm0hlcoxtd0ugzt4tbllb"
   },
   "source": [
    "Еще немного предобработки данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "623581a8",
   "metadata": {
    "cellId": "sj86nm9kj2mvwcg5yxcxh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Collater:\n",
    "    def __init__(self, \n",
    "                 tokenizer_name='sberbank-ai/rugpt3small_based_on_gpt2', \n",
    "                 max_length=24):\n",
    "        \n",
    "        # будем использовать предобученный токенизатор\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        # устанавливаем пад токен, он будет иметь индекс \"длина словаря + 1\"\n",
    "        self.tokenizer.pad_token = '<pad>'\n",
    "        self.tokenizer.bos_token = '<bos>'\n",
    "        self.tokenizer.eos_token = '<eos>'\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def tokenize(self, texts):\n",
    "        tokenized = self.tokenizer(texts, padding=True, \n",
    "                                   truncation=True,\n",
    "                                   max_length=self.max_length, \n",
    "                                   return_tensors='pt')\n",
    "        # за пару минут я не нашел как вставить пад на нужное мне место, поэтому прибегнул к такому хаку\n",
    "        tokenized['input_ids'] = tokenized['input_ids'] * tokenized['attention_mask']\n",
    "        return tokenized\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        questions, responses = list(), list()\n",
    "        \n",
    "        for el in batch:\n",
    "            #el[0]: str\n",
    "            questions.append('<bos> ' + el[0] + ' <eos>')\n",
    "            responses.append('<bos> ' + el[1] + ' <eos>')\n",
    "            \n",
    "        tokenized_questions = self.tokenize(questions)\n",
    "        tokenized_responses = self.tokenize(responses)\n",
    "        \n",
    "        return (tokenized_questions, tokenized_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ceed155e",
   "metadata": {
    "cellId": "xkikcezyprd3ody23qfwd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "512/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c5c1a659",
   "metadata": {
    "cellId": "bm0woxjz9sen0pq98nh3t"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "collater = Collater(tokenizer_name='sberbank-ai/rugpt3small_based_on_gpt2')\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True, collate_fn=collater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ebe777d",
   "metadata": {
    "cellId": "ej7s1uqa11rg4x33zu3g26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  32,   70, 1410,   34, 2839, 4415, 8483,   73, 1410,   34]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "collater.tokenize('<bos> Котик <eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "714edf52",
   "metadata": {
    "cellId": "qdxh0w3wmsk75atcgx1wrg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "Question batch shape: torch.Size([256, 24])\n",
      "Answer batch shape: torch.Size([256, 24])\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_out = next(iter(train_loader))\n",
    "print(train_out[1].keys())\n",
    "print(f\"Question batch shape: {train_out[0]['input_ids'].size()}\")\n",
    "print(f\"Answer batch shape: {train_out[1]['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f40d3a86",
   "metadata": {
    "cellId": "vqtbwai15dg9hf8l36efe"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from razdel import tokenize\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for pair in data_iter:\n",
    "        src_tokens, trg_tokens = list(tokenize(pair[0])), list(tokenize(pair[1]))\n",
    "        src_text_tokens, trg_text_tokens = [_.text for _ in src_tokens], [_.text for _ in trg_tokens]\n",
    "        src_text_tokens.extend(trg_text_tokens)\n",
    "        yield src_text_tokens\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=['<pad>', '<unk>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "44b37b72",
   "metadata": {
    "cellId": "bp93z923qih7y9sghd0ts"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 781, 25, 346, 1, 3, 1, 0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# test\n",
    "vocab([\"<bos>\", \"работает\", \"ли\", \"эта\", \"штукенция\", \"<eos>\", \"<unk>\", \"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5baae4a4",
   "metadata": {
    "cellId": "9pl2w3tq0daavhw62yewic"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "#test \n",
    "vocab.get_stoi()[\"работает\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4616901b",
   "metadata": {
    "cellId": "39ydf8kjcnkmac8xtebd4g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'пор'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "vocab.get_itos()[700]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c4902",
   "metadata": {
    "cellId": "31oby9voyz3ymxq0utm6v"
   },
   "source": [
    "Disclaimer:\n",
    "\n",
    "пользовалась вот этим туториалом https://www.youtube.com/watch?v=U0s0f995w14&ab_channel=AladdinPersson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "067f7883",
   "metadata": {
    "cellId": "5d7tf6x4gmjydqazez0pj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c884571",
   "metadata": {
    "cellId": "zi9xvlnhitdtryr9t3xe"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2df18fa0",
   "metadata": {
    "cellId": "c5wgxmf0r7g6d3ghfxs8he"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size,\n",
    "                 num_layers, heads, device,\n",
    "                 forward_expansion, dropout, max_length):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same, it's in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad8ba561",
   "metadata": {
    "cellId": "cadntt3j4u5bhi8mbzdahu"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size,\n",
    "                 embed_size, num_layers, heads,\n",
    "                 forward_expansion, dropout, device, max_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10656b87",
   "metadata": {
    "cellId": "ta9e8t6lhps804ckqhfn3"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n",
    "                 embed_size=512, num_layers=6, forward_expansion=4,\n",
    "                 heads=8, dropout=0, device=\"cpu\", max_length=100):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8482ca9",
   "metadata": {
    "cellId": "jmocdoyywfd0p0e8w3q4"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-68aa63ba32b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_pad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_pad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "# test\n",
    "\n",
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
    "out = model(x, trg[:, :-1])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898dceed",
   "metadata": {
    "cellId": "jirkncjboabcw3azsfjv9"
   },
   "source": [
    "* обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7131bd59",
   "metadata": {
    "cellId": "djdcozyqe18lsvcugj7vy"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "src_vocab_size = vocab_size\n",
    "trg_vocab_size = vocab_size\n",
    "src_pad_idx = vocab[\"<pad>\"]\n",
    "trg_pad_idx = vocab[\"<pad>\"]\n",
    "\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n",
    "                    embed_size=512, num_layers=3, forward_expansion=4,\n",
    "                    heads=8, dropout=0, device=device, max_length=100).to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate = 3e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "4aa6d843",
   "metadata": {
    "cellId": "ogis7spw299ksduaqn7hdl",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "for n_iter, (question, response) in enumerate(train_loader):\n",
    "    print(question['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a9bb24a",
   "metadata": {
    "cellId": "txigtpmxo0r180pzytlxj4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object find_substrings at 0x7f54ec0c8b30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "03202b14",
   "metadata": {
    "cellId": "vy2l3j9ssxeua7q1e6518"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from razdel import tokenize\n",
    "\n",
    "stoi = vocab.get_stoi()\n",
    "itos = vocab.get_itos()\n",
    "\n",
    "def translate_sentence(model, sentence, device, max_length=50):\n",
    "    # Create tokens nd everything in lower case (which is what our vocab is)\n",
    "    \n",
    "    print(sentence)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in list(tokenize(sentence))]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # Add <BOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, '<bos>')\n",
    "    tokens.append('<eos>')\n",
    "    print(tokens)\n",
    "\n",
    "    # Go through each src token and convert to an index\n",
    "    #text_to_indices = [stoi[token] for token in tokens]\n",
    "    \n",
    "    sentence_tensor = collater.tokenize(sentence)\n",
    "\n",
    "    # Convert to Tensor\n",
    "    #sentence_tensor = torch.LongTensor(text_to_indices['input_ids']).unsqueeze(1).to(device)\n",
    "\n",
    "    outputs = [stoi['<bos>']]\n",
    "    \n",
    "    trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(sentence_tensor['input_ids'], trg_tensor)\n",
    "\n",
    "    best_guess = output.argmax(2)[-1, :].item()\n",
    "    outputs.append(best_guess)\n",
    "\n",
    "    #    if best_guess == stoi['<eos>']:\n",
    "    #        break\n",
    "\n",
    "    translated_sentence = [itos[idx] for idx in outputs]\n",
    "    # remove start token\n",
    "    return translated_sentence[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "26bcb15f",
   "metadata": {
    "cellId": "u1tghmeus5kpr5iukv3pfg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/368 [02:58<?, ?it/s]\n",
      "/kernel/lib/python3.8/site-packages/ml_kernel/ignored_keyboard_interrupt.py:16: UserWarning: Kernel cannot be interrupted during state load\n",
      "  warnings.warn(self._warn_message)\n",
      "/kernel/lib/python3.8/site-packages/ml_kernel/ignored_keyboard_interrupt.py:16: UserWarning: Kernel cannot be interrupted during state load\n",
      "  warnings.warn(self._warn_message)\n",
      "/kernel/lib/python3.8/site-packages/ml_kernel/ignored_keyboard_interrupt.py:16: UserWarning: Kernel cannot be interrupted during state load\n",
      "  warnings.warn(self._warn_message)\n",
      "Epoch 1: 100%|██████████| 368/368 [05:05<00:00,  1.21it/s, loss=0.442]]\n",
      "Epoch 2: 100%|██████████| 368/368 [02:27<00:00,  2.49it/s, loss=0.119]\n",
      "Epoch 3: 100%|██████████| 368/368 [02:28<00:00,  2.48it/s, loss=0.044] \n",
      "Epoch 4: 100%|██████████| 368/368 [02:18<00:00,  3.05it/s, loss=0.0184]"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c484c7dd4426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'transformer_state_dict_{n_epoch}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kernel/lib/python3.8/site-packages/ml_kernel/kernel.py:862: UserWarning: The following variables cannot be serialized: progress_bar\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "collater = Collater(tokenizer_name='sberbank-ai/rugpt3small_based_on_gpt2')\n",
    "\n",
    "model.train()\n",
    "\n",
    "for n_epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    progress_bar = tqdm(total=len(train_loader), desc=f'Epoch {n_epoch}')\n",
    "    \n",
    "    sentence = 'В каком университете ты учишься?'\n",
    "    \n",
    "    #model.eval()\n",
    "    #translated_sentence = translate_sentence(\n",
    "    #    model, sentence, device, max_length=50\n",
    "    #)\n",
    "\n",
    "    #print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "    #model.train()\n",
    "    \n",
    "    losses = list()\n",
    "    \n",
    "    for n_iter, (question, response) in enumerate(train_loader):\n",
    "        \n",
    "        #question = batch_to_device(question, device)\n",
    "        #response = batch_to_device(response, device)\n",
    "        question = question['input_ids'].to(device)\n",
    "        response = response['input_ids'].to(device)\n",
    "        \n",
    "        #question_embeddings, response_embeddings = model(question, response)\n",
    "        response_embeddings = model(question, response)\n",
    "        #print(out.size())   # 512, 24, 117016 batch_size, max_size in collater, vocab_size\n",
    "        response_embeddings = response_embeddings.to(device)\n",
    "        #print(response_embeddings.size()) \n",
    "        \n",
    "        #idea\n",
    "        #outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        #trg_flatten = batch.trg[1:].view(-1)\n",
    "        \n",
    "        #bad idea\n",
    "        #question_embeddings = question_embeddings[1:].reshape(-1, question_embeddings.shape[2])\n",
    "        #response_embeddings = response_embeddings.reshape(response_embeddings.shape[0], -1)\n",
    "        \n",
    "        output = response_embeddings[1:].reshape(-1, response_embeddings.shape[2])\n",
    "        target = response[1:].reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(output.to(device), target.to(device))\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        \n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.update()\n",
    "        progress_bar.set_postfix(loss=np.mean(losses[-100:]))\n",
    "    \n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "    torch.save(model.state_dict(), f'transformer_state_dict_{n_epoch}.pth')\n",
    "\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed4fc8",
   "metadata": {
    "cellId": "1xqtagqt43tyyjkgf50c9"
   },
   "source": [
    "```\n",
    "Epoch 1: 100%|██████████| 368/368 [05:05<00:00,  1.21it/s, loss=0.442]]\n",
    "Epoch 2: 100%|██████████| 368/368 [02:27<00:00,  2.49it/s, loss=0.119]\n",
    "Epoch 3: 100%|██████████| 368/368 [02:28<00:00,  2.48it/s, loss=0.044] \n",
    "Epoch 4: 100%|██████████| 368/368 [02:18<00:00,  3.05it/s, loss=0.0184]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c7e46",
   "metadata": {
    "cellId": "9n1m3jd70v7m8388p6m8s"
   },
   "source": [
    "## Итого:\n",
    "    \n",
    "У меня получилось сделать один луп обучение модели на сокращенных данных, где я брала только каждое 50-ое вхождение из исходных данных. Еще размер батча вынужденно 256, потому что 512 не проходило по ограничением памяти. Делалось это всё в яндекс датасфере.\n",
    "\n",
    "Дальше надо было реализовать нормальное обучение модели, то есть взять побольше данных и оставить ее часа на два, но это сделать я, к сожалению, уже не успела.\n",
    "\n",
    "Метод генерации ответа, я написала, но не до конца его дотестировала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "048f28ad",
   "metadata": {
    "cellId": "kkybh4987gow92jw3bsjp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В каком университете ты учишься?\n",
      "['<bos>', 'в', 'каком', 'университете', 'ты', 'учишься', '?', '<eos>']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-75e4c818f9c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'В каком университете ты учишься?'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m translated_sentence = translate_sentence(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-44-b4616f9a8b25>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(model, sentence, device, max_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<bos>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kernel/lib/python3.8/site-packages/ml_kernel/kernel.py:862: UserWarning: The following variables cannot be serialized: model, optimizer, output, scheduler\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "model.eval()\n",
    "\n",
    "sentence = 'В каком университете ты учишься?'\n",
    "translated_sentence = translate_sentence(\n",
    "    model, sentence, device, max_length=50\n",
    ")\n",
    "\n",
    "print(f\"Translated example sentence: \\n {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "6a8661c8-231c-49d8-8447-e0a2cc23d630",
  "notebookPath": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
